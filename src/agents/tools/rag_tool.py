# src/agents/tools/rag_tool.py: ADK tool wrapping our RAG pipeline.

import logging
from pydantic import BaseModel, Field

from google.adk.tools import FunctionTool

# Use 'from src...' imports consistent with current setup
from src.core.qa_service import initialize_chat_engine, stream_chat_response # Correctly import stream_chat_response
from src.config import settings
from llama_index.core import Settings as LlamaSettings
from llama_index.llms.openai import OpenAI as LlamaIndexOpenAI

logger = logging.getLogger(__name__)
logging.basicConfig(level=settings.log_level)

# --- RAG Tool Pydantic Input/Output Models (Optional but good practice for ADK) ---
class RagQueryInput(BaseModel):
    query: str = Field(description="The question to ask the RAG document knowledge base.")

class RagToolOutput(BaseModel):
    answer: str = Field(description="The answer retrieved and generated by the RAG pipeline.")
    # We could add retrieved_sources here later if needed by the agent

# --- RAG Tool Implementation Function ---
# Global variable to hold the RAG chat engine instance for the tool's lifetime (or app lifetime if tool is singleton)
# This avoids re-initializing the engine (and reloading the index) on every tool call.
_rag_chat_engine = None

def get_or_initialize_rag_engine():
    """Initializes the RAG chat engine if not already done, and returns it."""
    global _rag_chat_engine
    if _rag_chat_engine is None:
        logger.info("RAG Tool: No active RAG chat engine. Initializing...")
        
        original_llm = LlamaSettings.llm
        original_embed_model = LlamaSettings.embed_model # Store original embed model too

        rag_llm_model_name = settings.openai_compatible_llm_model_name
        logger.info(f"RAG Tool: Temporarily setting LlamaSettings.llm to {rag_llm_model_name} for RAG engine init.")
        LlamaSettings.llm = LlamaIndexOpenAI(
            model=rag_llm_model_name,
            temperature=settings.temperature,
            api_key=settings.openai_api_key,
            max_tokens=settings.max_tokens
        )
        # Ensure the correct embedding model is also set for RAG context
        # This typically happens in configure_llama_index_globals, but good to be explicit if tool init is isolated.
        from llama_index.core.embeddings import resolve_embed_model
        LlamaSettings.embed_model = resolve_embed_model(f"local:{settings.embedding_model_name}")
        
        _rag_chat_engine = initialize_chat_engine() # initialize_chat_engine uses global LlamaSettings
        
        LlamaSettings.llm = original_llm
        LlamaSettings.embed_model = original_embed_model # Restore original embed model
        logger.info(f"RAG Tool: Restored LlamaSettings.llm to {original_llm.model if hasattr(original_llm, 'model') else original_llm} and embed_model.")

        if not _rag_chat_engine:
            logger.error("RAG Tool: Failed to initialize RAG chat engine.")
            raise RuntimeError("RAG chat engine could not be initialized for the tool.")
    return _rag_chat_engine

def query_rag_documents(rag_input: RagQueryInput) -> RagToolOutput:
    """Tool function to query the local RAG document knowledge base."""
    logger.info(f"RAG Tool received query: {rag_input.query}")
    try:
        engine = get_or_initialize_rag_engine()
        
        response_content = []
        for token in stream_chat_response(rag_input.query, engine):
            response_content.append(token)
        answer = "".join(response_content)

        logger.info(f"RAG Tool produced answer (first 100 chars): {answer[:100]}...")
        return RagToolOutput(answer=answer)
    except Exception as e:
        logger.error(f"Error in RAG Tool query: {e}", exc_info=True)
        return RagToolOutput(answer="Sorry, I encountered an error trying to query the documents.")

# Create the ADK FunctionTool
rag_document_tool = FunctionTool.from_function(
    fn=query_rag_documents,
    name="QueryLocalDocuments",
    description="Queries a local knowledge base of indexed documents to answer questions or find specific information. Use for questions about internal project documents, specific PDFs, or user-uploaded content.",
    args_schema=RagQueryInput, # Explicitly providing schema for clarity
    output_schema=RagToolOutput
)

if __name__ == "__main__":
    # Test the tool directly
    # Ensure API keys and index are set up
    import sys
    from pathlib import Path
    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT) not in sys.path:
        sys.path.insert(0, str(PROJECT_ROOT))
    
    # Ensure LlamaIndex globals (especially embed_model) are configured
    from src.core.indexing_service import configure_llama_index_globals as core_configure_globals, get_active_settings
    # We need to configure LlamaSettings with the RAG-specific models for this test.
    # The get_or_initialize_rag_engine will do this temporarily, but for a standalone test,
    # it's good to ensure the global context is generally what RAG expects initially.
    temp_rag_settings = get_active_settings() # Get current general settings
    temp_rag_settings.llm_model_name = settings.openai_compatible_llm_model_name # Override for RAG test
    core_configure_globals(temp_rag_settings) 
    
    logger.info("--- RAG Tool Test --- ")    
    if not settings.openai_api_key or settings.openai_api_key == "your_openai_api_key_here_if_not_in_env":
        logger.warning("OPENAI_API_KEY not found or is default. RAG tool test might not get proper LLM responses.")

    # Example tool invocation
    test_query = "Tell me about the mechanical data of speedmate"
    # test_input = RagQueryInput(query=test_query) # This is for direct call
    test_input_dict = {"query": test_query} # ADK FunctionTool.call expects a dict
    
    print(f"Testing RAG tool with query: '{test_query}'")
    try:
        output_dict = rag_document_tool.call(test_input_dict)
        print(f"RAG Tool Test Output (dict): {output_dict}")
        if output_dict and 'answer' in output_dict:
             print(f"Answer: {output_dict['answer']}")
        else:
            print("Failed to get a structured answer from the tool call.")
    except Exception as e:
        print(f"Error during RAG tool test: {e}", exc_info=True)

    logger.info("--- RAG Tool Test Complete ---") 