# Pydantic models for data validation and structured data handling.

from pydantic import BaseModel, Field
from typing import List, Optional, Literal
from uuid import UUID, uuid4
from datetime import datetime
from dataclasses import dataclass, field
from datetime import timezone

# Example Pydantic model (can be expanded later)
class DocumentMetadata(BaseModel):
    source: str = Field(description="The source identifier of the document, e.g., file path or URL")
    page_number: Optional[int] = Field(None, description="Page number if applicable")
    last_modified: Optional[str] = Field(None, description="Last modified timestamp of the document")

class Chunk(BaseModel):
    text: str = Field(description="The text content of the chunk")
    metadata: DocumentMetadata = Field(description="Metadata associated with the chunk")
    embedding: Optional[List[float]] = Field(None, description="The vector embedding of the chunk text")

class Query(BaseModel):
    question: str = Field(description="The user's query")
    chat_history: Optional[List[dict]] = Field(None, description="Previous conversation turns")

class Answer(BaseModel):
    answer: str = Field(description="The LLM generated answer")
    retrieved_contexts: Optional[List[Chunk]] = Field(None, description="List of retrieved context chunks used for the answer")
    source_documents: Optional[List[str]] = Field(None, description="List of source document identifiers relevant to the answer")

class DocumentCitation(BaseModel):
    """
    Citation reference to a specific document chunk or source used in RAG response generation.
    
    This model represents a single citation that links an AI-generated response back to
    its source material. Each citation identifies a specific document chunk that contributed
    to the answer, enabling users to verify information and explore source materials.
    
    Citations are automatically generated during the RAG process by analyzing which
    document chunks were retrieved and used by the language model for response synthesis.
    The snippet provides a preview of the relevant content, while the full document
    details enable precise source attribution.
    
    Used for:
        - Source attribution and transparency
        - Fact verification and validation  
        - User exploration of source materials
        - Response quality assessment
    """
    document_id: UUID = Field(default_factory=uuid4, description="Unique identifier for the cited document chunk or reference.")
    document_name: Optional[str] = Field(None, description="Name of the document, derived from filename (e.g., 'LMS24_en').")
    document_title: Optional[str] = Field(None, description="Extracted or derived title of the document.")
    file_path: Optional[str] = Field(None, description="Relative file path to the document (e.g., 'data/LMS24_en.pdf').")
    page_label: Optional[str] = Field(None, description="Page number or label within the document where the information was found.")
    snippet: Optional[str] = Field(None, description="A short snippet of text from the document relevant to the answer (first ~200 chars).")

# New model for structured RAG response output
class RAGSourceNodeDetail(BaseModel):
    node_id: str
    file_path: Optional[str]
    page_label: Optional[str]
    score: Optional[float] = Field(None, description="Normalized relevance score (e.g., sigmoid of reranker logit), typically 0 to 1. Higher is more relevant.")
    node_title: Optional[str] # Title generated by LLM for the node/document
    full_text_content: str = Field(description="Full text content of the source node")

class RAGResponse(BaseModel):
    """
    Comprehensive structured output from a RAG (Retrieval-Augmented Generation) query.
    
    This model captures the complete lifecycle of a RAG query, from initial retrieval
    through final response generation. It includes timing metrics, configuration details,
    source attribution, and the final synthesized answer, providing full observability
    into the RAG process for debugging, optimization, and quality assessment.
    
    The model serves multiple purposes:
        - Performance monitoring and optimization
        - Source attribution and citation generation
        - Quality assessment and debugging
        - Configuration tracking and reproducibility
        - User transparency and trust building
    
    All timestamps are in UTC for consistency across different deployment environments.
    The configuration fields capture the exact settings used for this specific query,
    enabling reproducible results and systematic performance analysis.
    """
    query_time_utc: datetime = Field(description="Timestamp when the query was received (UTC)")
    response_time_utc: datetime = Field(description="Timestamp when the response generation was completed (UTC)")
    processing_duration_seconds: float = Field(description="Total time taken from query receipt to response completion")
    original_query: str = Field(description="The original query submitted by the user")
    
    llm_model_used: str = Field(description="Identifier of the LLM used for synthesis (e.g., gpt-4o-mini)")
    embedding_model_name_config: str = Field(description="Embedding model name from configuration")
    retriever_similarity_top_k_config: int = Field(description="Configured similarity_top_k for the initial retriever")
    reranker_model_name_config: str = Field(description="Reranker model name from configuration")
    reranker_top_n_config: int = Field(description="Configured top_n for the reranker")
    
    source_nodes_count_after_reranking: int = Field(description="Number of source nodes remaining after reranking")
    source_nodes_details: List[RAGSourceNodeDetail] = Field(description="Details of the source nodes used for the response, after reranking")
    
    final_answer_text: str = Field(description="The complete text of the generated answer")
    citations_generated: List[DocumentCitation] = Field(description="List of document citations generated for the answer")
    bm25_raw_retrieved_snippets: Optional[List[str]] = Field(None, description="Snippets from top N raw BM25 results before fusion/reranking, for observability.") 
    expanded_queries: Optional[List[str]] = Field(None, description="Queries generated by the query expansion step, if any, including the original.")

@dataclass
class ConversationTurn:
    """Represents a single conversation turn."""
    user_query: str
    ai_response: str
    timestamp: datetime
    query_type: str = "standalone"  # "standalone" or "contextual"

@dataclass
class ConversationMemory:
    """In-session conversation memory for RAG context enhancement."""
    turns: List[ConversationTurn] = field(default_factory=list)
    max_turns: int = 10  # Keep last N turns for context
    
    def add_turn(self, user_query: str, ai_response: str, query_type: str = "standalone") -> None:
        """Add a new conversation turn."""
        turn = ConversationTurn(
            user_query=user_query,
            ai_response=ai_response,
            timestamp=datetime.now(timezone.utc),
            query_type=query_type
        )
        self.turns.append(turn)
        
        # Keep only last max_turns
        if len(self.turns) > self.max_turns:
            self.turns = self.turns[-self.max_turns:]
    
    def get_recent_context(self, num_turns: int = 3) -> List[ConversationTurn]:
        """Get recent conversation turns for context."""
        return self.turns[-num_turns:] if self.turns else []
    
    def get_conversation_history_text(self, num_turns: int = 3) -> str:
        """Get formatted conversation history for LLM prompts."""
        recent_turns = self.get_recent_context(num_turns)
        if not recent_turns:
            return ""
        
        history_parts = []
        for turn in recent_turns:
            history_parts.append(f"User: {turn.user_query}")
            history_parts.append(f"Assistant: {turn.ai_response}")
        
        return "\n".join(history_parts)

@dataclass 
class QueryEnhancementRequest:
    """Request for query enhancement with conversation context."""
    current_query: str
    conversation_history: str

# New Pydantic models for LLM-based query enhancement
class QueryEnhancementAssessment(BaseModel):
    """
    LLM assessment of query enhancement needs for conversational RAG.
    
    This model represents the structured response from an LLM that analyzes
    whether a user's query needs contextual enhancement based on conversation
    history. The LLM determines if the query is self-contained (standalone)
    or requires context from previous conversation turns (contextual).
    
    For contextual queries, the LLM creates an enhanced version that incorporates
    relevant context to make the query suitable for semantic search and document
    retrieval, ensuring the embedding model receives a complete, self-contained
    query rather than one with ambiguous references.
    
    Examples:
        Standalone: "What is the company's revenue for 2023?" 
            -> No enhancement needed, query is complete
        Contextual: "What about last quarter?" (after discussing revenue)
            -> Enhanced to "What was the company's revenue for last quarter?"
    """
    query_type: Literal["standalone", "contextual"] = Field(
        description="Classification of query type: 'standalone' if query is complete and self-contained, 'contextual' if it relies on previous conversation elements like pronouns, references, or implied subjects"
    )
    needs_enhancement: bool = Field(
        description="Boolean flag indicating whether the query requires enhancement with conversation context to be properly understood and retrieved against documents"
    )
    enhanced_query: str = Field(
        description="The enhanced query optimized for semantic search and document retrieval. For standalone queries, this will be identical to the original. For contextual queries, this incorporates relevant context to make the query self-contained and unambiguous"
    )
    reasoning: str = Field(
        description="Brief explanation of the assessment decision, including what contextual elements were identified (if any) and how they were incorporated into the enhanced query"
    )
    confidence: float = Field(
        ge=0.0, le=1.0, 
        description="Confidence score for the assessment accuracy on a scale from 0.0 (low confidence) to 1.0 (high confidence). Higher values indicate the LLM is more certain about its classification and enhancement"
    )

@dataclass
class QueryEnhancementResult:
    """Result of query enhancement process."""
    original_query: str
    enhanced_query: str
    is_contextual: bool
    reasoning: str
    confidence: float 